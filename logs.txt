
==> Audit <==
|-----------|-----------------------|----------|---------|---------|---------------------|---------------------|
|  Command  |         Args          | Profile  |  User   | Version |     Start Time      |      End Time       |
|-----------|-----------------------|----------|---------|---------|---------------------|---------------------|
| start     |                       | minikube | febrian | v1.33.1 | 14 May 24 10:45 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 14 May 24 10:53 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 14 May 24 11:08 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 14 May 24 11:32 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 14 May 24 12:50 WIB |                     |
| delete    |                       | minikube | febrian | v1.33.1 | 14 May 24 13:07 WIB | 14 May 24 13:07 WIB |
| start     |                       | minikube | febrian | v1.33.1 | 14 May 24 13:08 WIB | 14 May 24 13:09 WIB |
| dashboard |                       | minikube | febrian | v1.33.1 | 14 May 24 13:11 WIB |                     |
| service   | hello-node            | minikube | febrian | v1.33.1 | 14 May 24 13:20 WIB | 14 May 24 13:21 WIB |
| addons    | enable metrics-server | minikube | febrian | v1.33.1 | 14 May 24 13:24 WIB | 14 May 24 13:24 WIB |
| service   | hello-node            | minikube | febrian | v1.33.1 | 14 May 24 13:37 WIB | 14 May 24 13:38 WIB |
| service   | hello-node            | minikube | febrian | v1.33.1 | 14 May 24 13:38 WIB | 14 May 24 13:39 WIB |
| service   | spring-petclinic-rest | minikube | febrian | v1.33.1 | 14 May 24 14:18 WIB | 14 May 24 14:20 WIB |
| dashboard |                       | minikube | febrian | v1.33.1 | 14 May 24 14:25 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 14 May 24 14:26 WIB | 14 May 24 14:27 WIB |
| dashboard |                       | minikube | febrian | v1.33.1 | 14 May 24 14:27 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 14 May 24 14:32 WIB | 14 May 24 14:33 WIB |
| dashboard |                       | minikube | febrian | v1.33.1 | 14 May 24 14:37 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 14 May 24 14:37 WIB | 14 May 24 14:38 WIB |
| dashboard |                       | minikube | febrian | v1.33.1 | 14 May 24 14:39 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 17 May 24 00:03 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 17 May 24 00:03 WIB | 17 May 24 00:04 WIB |
| dashboard |                       | minikube | febrian | v1.33.1 | 17 May 24 00:04 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 17 May 24 00:10 WIB | 17 May 24 00:10 WIB |
| dashboard |                       | minikube | febrian | v1.33.1 | 17 May 24 00:10 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 17 May 24 01:32 WIB | 17 May 24 01:32 WIB |
| dashboard |                       | minikube | febrian | v1.33.1 | 17 May 24 01:32 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 17 May 24 18:59 WIB | 17 May 24 18:59 WIB |
| start     |                       | minikube | febrian | v1.33.1 | 17 May 24 19:40 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 17 May 24 19:45 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 17 May 24 19:54 WIB |                     |
| start     |                       | minikube | febrian | v1.33.1 | 17 May 24 20:09 WIB |                     |
|-----------|-----------------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/05/17 20:09:18
Running on machine: Macbook
Binary: Built with gc go1.22.3 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0517 20:09:18.772125    5139 out.go:291] Setting OutFile to fd 1 ...
I0517 20:09:18.772768    5139 out.go:343] isatty.IsTerminal(1) = true
I0517 20:09:18.772770    5139 out.go:304] Setting ErrFile to fd 2...
I0517 20:09:18.772774    5139 out.go:343] isatty.IsTerminal(2) = true
I0517 20:09:18.773300    5139 root.go:338] Updating PATH: /Users/febrian/.minikube/bin
I0517 20:09:18.777305    5139 out.go:298] Setting JSON to false
I0517 20:09:18.805042    5139 start.go:129] hostinfo: {"hostname":"Macbook.lan","uptime":10561,"bootTime":1715940797,"procs":441,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.4","kernelVersion":"23.4.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"da8f5a95-397d-543e-9dcb-926e75fa226c"}
W0517 20:09:18.805158    5139 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0517 20:09:18.814831    5139 out.go:177] üòÑ  minikube v1.33.1 on Darwin 14.4 (arm64)
I0517 20:09:18.825218    5139 out.go:177]     ‚ñ™ KUBECONFIG=/etc/kubernetes/admin.conf
I0517 20:09:18.825932    5139 notify.go:220] Checking for updates...
I0517 20:09:18.830846    5139 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0517 20:09:18.831624    5139 driver.go:392] Setting default libvirt URI to qemu:///system
I0517 20:09:19.002197    5139 docker.go:122] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I0517 20:09:19.002490    5139 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0517 20:09:19.305647    5139 info.go:266] docker info: {ID:fbd9d779-5c74-44ed-903a-7934fa9ef571 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:66 OomKillDisable:false NGoroutines:82 SystemTime:2024-05-17 13:09:19.286683551 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:6.6.26-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4113440768 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/febrian/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/febrian/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/Users/febrian/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/Users/febrian/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/Users/febrian/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/febrian/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/febrian/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/febrian/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/febrian/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/febrian/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0517 20:09:19.310686    5139 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0517 20:09:19.318830    5139 start.go:297] selected driver: docker
I0517 20:09:19.318836    5139 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0517 20:09:19.319053    5139 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0517 20:09:19.319164    5139 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0517 20:09:19.415917    5139 info.go:266] docker info: {ID:fbd9d779-5c74-44ed-903a-7934fa9ef571 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:66 OomKillDisable:false NGoroutines:82 SystemTime:2024-05-17 13:09:19.401270217 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:6.6.26-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4113440768 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/febrian/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/febrian/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/Users/febrian/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/Users/febrian/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/Users/febrian/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/febrian/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/febrian/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/febrian/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/febrian/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/febrian/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0517 20:09:19.416597    5139 cni.go:84] Creating CNI manager for ""
I0517 20:09:19.416617    5139 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0517 20:09:19.416826    5139 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0517 20:09:19.425624    5139 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0517 20:09:19.428532    5139 cache.go:121] Beginning downloading kic base image for docker with docker
I0517 20:09:19.432579    5139 out.go:177] üöú  Pulling base image v0.0.44 ...
I0517 20:09:19.439591    5139 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0517 20:09:19.439605    5139 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0517 20:09:19.439654    5139 preload.go:147] Found local preload: /Users/febrian/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0517 20:09:19.439850    5139 cache.go:56] Caching tarball of preloaded images
I0517 20:09:19.440578    5139 preload.go:173] Found /Users/febrian/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0517 20:09:19.440597    5139 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0517 20:09:19.440688    5139 profile.go:143] Saving config to /Users/febrian/.minikube/profiles/minikube/config.json ...
I0517 20:09:19.482308    5139 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0517 20:09:19.482330    5139 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0517 20:09:19.482347    5139 cache.go:194] Successfully downloaded all kic artifacts
I0517 20:09:19.482963    5139 start.go:360] acquireMachinesLock for minikube: {Name:mk529743d6d086e43fdeebe15374fb60ecfaddc1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0517 20:09:19.483108    5139 start.go:364] duration metric: took 119.583¬µs to acquireMachinesLock for "minikube"
I0517 20:09:19.483128    5139 start.go:96] Skipping create...Using existing machine configuration
I0517 20:09:19.483241    5139 fix.go:54] fixHost starting: 
I0517 20:09:19.483473    5139 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0517 20:09:19.522725    5139 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0517 20:09:19.522752    5139 fix.go:138] unexpected machine state, will restart: <nil>
I0517 20:09:19.530545    5139 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I0517 20:09:19.533572    5139 machine.go:94] provisionDockerMachine start ...
I0517 20:09:19.533955    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:19.575870    5139 main.go:141] libmachine: Using SSH client type: native
I0517 20:09:19.576727    5139 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10318f180] 0x1031919e0 <nil>  [] 0s} 127.0.0.1 51860 <nil> <nil>}
I0517 20:09:19.576731    5139 main.go:141] libmachine: About to run SSH command:
hostname
I0517 20:09:19.745405    5139 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0517 20:09:19.745451    5139 ubuntu.go:169] provisioning hostname "minikube"
I0517 20:09:19.745737    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:19.788818    5139 main.go:141] libmachine: Using SSH client type: native
I0517 20:09:19.788967    5139 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10318f180] 0x1031919e0 <nil>  [] 0s} 127.0.0.1 51860 <nil> <nil>}
I0517 20:09:19.788970    5139 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0517 20:09:19.965578    5139 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0517 20:09:19.965749    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:20.011712    5139 main.go:141] libmachine: Using SSH client type: native
I0517 20:09:20.011867    5139 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10318f180] 0x1031919e0 <nil>  [] 0s} 127.0.0.1 51860 <nil> <nil>}
I0517 20:09:20.011874    5139 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0517 20:09:20.153804    5139 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0517 20:09:20.153830    5139 ubuntu.go:175] set auth options {CertDir:/Users/febrian/.minikube CaCertPath:/Users/febrian/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/febrian/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/febrian/.minikube/machines/server.pem ServerKeyPath:/Users/febrian/.minikube/machines/server-key.pem ClientKeyPath:/Users/febrian/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/febrian/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/febrian/.minikube}
I0517 20:09:20.153882    5139 ubuntu.go:177] setting up certificates
I0517 20:09:20.153894    5139 provision.go:84] configureAuth start
I0517 20:09:20.154127    5139 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0517 20:09:20.199594    5139 provision.go:143] copyHostCerts
I0517 20:09:20.200252    5139 exec_runner.go:144] found /Users/febrian/.minikube/ca.pem, removing ...
I0517 20:09:20.200414    5139 exec_runner.go:203] rm: /Users/febrian/.minikube/ca.pem
I0517 20:09:20.200530    5139 exec_runner.go:151] cp: /Users/febrian/.minikube/certs/ca.pem --> /Users/febrian/.minikube/ca.pem (1082 bytes)
I0517 20:09:20.201029    5139 exec_runner.go:144] found /Users/febrian/.minikube/cert.pem, removing ...
I0517 20:09:20.201032    5139 exec_runner.go:203] rm: /Users/febrian/.minikube/cert.pem
I0517 20:09:20.201128    5139 exec_runner.go:151] cp: /Users/febrian/.minikube/certs/cert.pem --> /Users/febrian/.minikube/cert.pem (1123 bytes)
I0517 20:09:20.201396    5139 exec_runner.go:144] found /Users/febrian/.minikube/key.pem, removing ...
I0517 20:09:20.201398    5139 exec_runner.go:203] rm: /Users/febrian/.minikube/key.pem
I0517 20:09:20.201447    5139 exec_runner.go:151] cp: /Users/febrian/.minikube/certs/key.pem --> /Users/febrian/.minikube/key.pem (1675 bytes)
I0517 20:09:20.201958    5139 provision.go:117] generating server cert: /Users/febrian/.minikube/machines/server.pem ca-key=/Users/febrian/.minikube/certs/ca.pem private-key=/Users/febrian/.minikube/certs/ca-key.pem org=febrian.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0517 20:09:20.307335    5139 provision.go:177] copyRemoteCerts
I0517 20:09:20.307785    5139 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0517 20:09:20.307825    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:20.345390    5139 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51860 SSHKeyPath:/Users/febrian/.minikube/machines/minikube/id_rsa Username:docker}
I0517 20:09:20.461055    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0517 20:09:20.483465    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0517 20:09:20.494897    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0517 20:09:20.507311    5139 provision.go:87] duration metric: took 353.411208ms to configureAuth
I0517 20:09:20.507322    5139 ubuntu.go:193] setting minikube options for container-runtime
I0517 20:09:20.507467    5139 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0517 20:09:20.507549    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:20.547720    5139 main.go:141] libmachine: Using SSH client type: native
I0517 20:09:20.547862    5139 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10318f180] 0x1031919e0 <nil>  [] 0s} 127.0.0.1 51860 <nil> <nil>}
I0517 20:09:20.547865    5139 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0517 20:09:20.682156    5139 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0517 20:09:20.682178    5139 ubuntu.go:71] root file system type: overlay
I0517 20:09:20.682359    5139 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0517 20:09:20.682531    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:20.728293    5139 main.go:141] libmachine: Using SSH client type: native
I0517 20:09:20.728441    5139 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10318f180] 0x1031919e0 <nil>  [] 0s} 127.0.0.1 51860 <nil> <nil>}
I0517 20:09:20.728476    5139 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0517 20:09:20.899897    5139 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0517 20:09:20.900043    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:20.944939    5139 main.go:141] libmachine: Using SSH client type: native
I0517 20:09:20.945097    5139 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10318f180] 0x1031919e0 <nil>  [] 0s} 127.0.0.1 51860 <nil> <nil>}
I0517 20:09:20.945104    5139 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0517 20:09:21.108763    5139 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0517 20:09:21.108778    5139 machine.go:97] duration metric: took 1.575206s to provisionDockerMachine
I0517 20:09:21.108789    5139 start.go:293] postStartSetup for "minikube" (driver="docker")
I0517 20:09:21.108801    5139 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0517 20:09:21.108980    5139 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0517 20:09:21.109063    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:21.153790    5139 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51860 SSHKeyPath:/Users/febrian/.minikube/machines/minikube/id_rsa Username:docker}
I0517 20:09:21.265378    5139 ssh_runner.go:195] Run: cat /etc/os-release
I0517 20:09:21.270390    5139 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0517 20:09:21.270424    5139 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0517 20:09:21.270435    5139 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0517 20:09:21.270441    5139 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0517 20:09:21.270450    5139 filesync.go:126] Scanning /Users/febrian/.minikube/addons for local assets ...
I0517 20:09:21.270667    5139 filesync.go:126] Scanning /Users/febrian/.minikube/files for local assets ...
I0517 20:09:21.270800    5139 start.go:296] duration metric: took 162.003083ms for postStartSetup
I0517 20:09:21.271026    5139 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0517 20:09:21.271147    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:21.314131    5139 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51860 SSHKeyPath:/Users/febrian/.minikube/machines/minikube/id_rsa Username:docker}
I0517 20:09:21.423012    5139 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0517 20:09:21.428581    5139 fix.go:56] duration metric: took 1.945449209s for fixHost
I0517 20:09:21.428596    5139 start.go:83] releasing machines lock for "minikube", held for 1.945488541s
I0517 20:09:21.428739    5139 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0517 20:09:21.474210    5139 ssh_runner.go:195] Run: cat /version.json
I0517 20:09:21.474267    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:21.475616    5139 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0517 20:09:21.475697    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0517 20:09:21.521384    5139 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51860 SSHKeyPath:/Users/febrian/.minikube/machines/minikube/id_rsa Username:docker}
I0517 20:09:21.521334    5139 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51860 SSHKeyPath:/Users/febrian/.minikube/machines/minikube/id_rsa Username:docker}
I0517 20:09:22.548161    5139 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.072512291s)
I0517 20:09:22.548161    5139 ssh_runner.go:235] Completed: cat /version.json: (1.073905042s)
I0517 20:09:22.548635    5139 ssh_runner.go:195] Run: systemctl --version
I0517 20:09:22.557819    5139 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0517 20:09:22.562501    5139 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0517 20:09:22.574080    5139 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0517 20:09:22.574186    5139 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0517 20:09:22.578575    5139 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0517 20:09:22.578955    5139 start.go:494] detecting cgroup driver to use...
I0517 20:09:22.578972    5139 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0517 20:09:22.579373    5139 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0517 20:09:22.587176    5139 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0517 20:09:22.592375    5139 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0517 20:09:22.596924    5139 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0517 20:09:22.596996    5139 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0517 20:09:22.601653    5139 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0517 20:09:22.606122    5139 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0517 20:09:22.610777    5139 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0517 20:09:22.615620    5139 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0517 20:09:22.620383    5139 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0517 20:09:22.624945    5139 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0517 20:09:22.629448    5139 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0517 20:09:22.634306    5139 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0517 20:09:22.638941    5139 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0517 20:09:22.643087    5139 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0517 20:09:22.681903    5139 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0517 20:09:22.748109    5139 start.go:494] detecting cgroup driver to use...
I0517 20:09:22.748130    5139 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0517 20:09:22.748321    5139 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0517 20:09:22.763737    5139 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0517 20:09:22.763907    5139 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0517 20:09:22.779697    5139 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0517 20:09:22.788802    5139 ssh_runner.go:195] Run: which cri-dockerd
I0517 20:09:22.791228    5139 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0517 20:09:22.795434    5139 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0517 20:09:22.805292    5139 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0517 20:09:22.847577    5139 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0517 20:09:22.908598    5139 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0517 20:09:22.908767    5139 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0517 20:09:22.918685    5139 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0517 20:09:22.982887    5139 ssh_runner.go:195] Run: sudo systemctl restart docker
I0517 20:09:23.211109    5139 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0517 20:09:23.216410    5139 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0517 20:09:23.222695    5139 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0517 20:09:23.227948    5139 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0517 20:09:23.261030    5139 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0517 20:09:23.299895    5139 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0517 20:09:23.332819    5139 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0517 20:09:23.347493    5139 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0517 20:09:23.352962    5139 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0517 20:09:23.381523    5139 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0517 20:09:23.555313    5139 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0517 20:09:23.556247    5139 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0517 20:09:23.558475    5139 start.go:562] Will wait 60s for crictl version
I0517 20:09:23.558566    5139 ssh_runner.go:195] Run: which crictl
I0517 20:09:23.560509    5139 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0517 20:09:23.661353    5139 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0517 20:09:23.661527    5139 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0517 20:09:23.739552    5139 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0517 20:09:23.762974    5139 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0517 20:09:23.763398    5139 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0517 20:09:23.904572    5139 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0517 20:09:23.905204    5139 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0517 20:09:23.907852    5139 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0517 20:09:23.915705    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0517 20:09:23.962807    5139 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0517 20:09:23.962902    5139 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0517 20:09:23.962959    5139 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0517 20:09:23.977445    5139 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5
springcommunity/spring-petclinic-rest:3.0.2

-- /stdout --
I0517 20:09:23.977452    5139 docker.go:615] Images already preloaded, skipping extraction
I0517 20:09:23.977681    5139 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0517 20:09:23.986938    5139 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5
springcommunity/spring-petclinic-rest:3.0.2

-- /stdout --
I0517 20:09:23.986950    5139 cache_images.go:84] Images are preloaded, skipping loading
I0517 20:09:23.986959    5139 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0517 20:09:23.987099    5139 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0517 20:09:23.987181    5139 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0517 20:09:24.115065    5139 cni.go:84] Creating CNI manager for ""
I0517 20:09:24.115087    5139 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0517 20:09:24.115099    5139 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0517 20:09:24.115123    5139 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0517 20:09:24.115366    5139 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0517 20:09:24.115533    5139 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0517 20:09:24.120486    5139 binaries.go:44] Found k8s binaries, skipping transfer
I0517 20:09:24.120654    5139 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0517 20:09:24.124729    5139 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0517 20:09:24.133225    5139 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0517 20:09:24.141027    5139 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0517 20:09:24.149026    5139 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0517 20:09:24.151069    5139 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0517 20:09:24.156215    5139 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0517 20:09:24.191173    5139 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0517 20:09:24.207056    5139 certs.go:68] Setting up /Users/febrian/.minikube/profiles/minikube for IP: 192.168.49.2
I0517 20:09:24.207222    5139 certs.go:194] generating shared ca certs ...
I0517 20:09:24.207240    5139 certs.go:226] acquiring lock for ca certs: {Name:mka3206ee43fa200eab78487a4c3a01758a019af Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0517 20:09:24.208249    5139 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/febrian/.minikube/ca.key
I0517 20:09:24.208614    5139 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/febrian/.minikube/proxy-client-ca.key
I0517 20:09:24.208727    5139 certs.go:256] generating profile certs ...
I0517 20:09:24.209129    5139 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/febrian/.minikube/profiles/minikube/client.key
I0517 20:09:24.209420    5139 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/febrian/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0517 20:09:24.209692    5139 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/febrian/.minikube/profiles/minikube/proxy-client.key
I0517 20:09:24.210073    5139 certs.go:484] found cert: /Users/febrian/.minikube/certs/ca-key.pem (1675 bytes)
I0517 20:09:24.210268    5139 certs.go:484] found cert: /Users/febrian/.minikube/certs/ca.pem (1082 bytes)
I0517 20:09:24.210322    5139 certs.go:484] found cert: /Users/febrian/.minikube/certs/cert.pem (1123 bytes)
I0517 20:09:24.210533    5139 certs.go:484] found cert: /Users/febrian/.minikube/certs/key.pem (1675 bytes)
I0517 20:09:24.212817    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0517 20:09:24.226256    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0517 20:09:24.238937    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0517 20:09:24.251732    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0517 20:09:24.263249    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0517 20:09:24.275076    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0517 20:09:24.286813    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0517 20:09:24.299316    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0517 20:09:24.311295    5139 ssh_runner.go:362] scp /Users/febrian/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0517 20:09:24.328999    5139 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0517 20:09:24.341422    5139 ssh_runner.go:195] Run: openssl version
I0517 20:09:24.348236    5139 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0517 20:09:24.356227    5139 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0517 20:09:24.360006    5139 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 14 03:47 /usr/share/ca-certificates/minikubeCA.pem
I0517 20:09:24.360098    5139 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0517 20:09:24.367600    5139 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0517 20:09:24.375543    5139 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0517 20:09:24.382102    5139 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0517 20:09:24.389717    5139 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0517 20:09:24.393769    5139 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0517 20:09:24.399551    5139 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0517 20:09:24.405659    5139 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0517 20:09:24.410985    5139 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0517 20:09:24.416268    5139 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0517 20:09:24.416437    5139 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0517 20:09:24.433722    5139 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0517 20:09:24.440300    5139 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0517 20:09:24.440312    5139 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0517 20:09:24.440315    5139 kubeadm.go:587] restartPrimaryControlPlane start ...
I0517 20:09:24.440395    5139 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0517 20:09:24.445279    5139 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0517 20:09:24.445366    5139 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0517 20:09:24.500258    5139 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /etc/kubernetes/admin.conf
I0517 20:09:24.500316    5139 kubeconfig.go:62] /etc/kubernetes/admin.conf needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
W0517 20:09:24.500735    5139 kubeadm.go:610] unable to update kubeconfig (cluster will likely require a reset): write kubeconfig: Error creating directory: /etc/kubernetes: mkdir /etc/kubernetes: permission denied
I0517 20:09:24.501081    5139 kubeadm.go:591] duration metric: took 60.757958ms to restartPrimaryControlPlane
W0517 20:09:24.501361    5139 out.go:239] ü§¶  Unable to restart control-plane node(s), will reset cluster: <no value>
I0517 20:09:24.501387    5139 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0517 20:09:27.586149    5139 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (3.084175083s)
I0517 20:09:27.586350    5139 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0517 20:09:27.594030    5139 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0517 20:09:27.598943    5139 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0517 20:09:27.599105    5139 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0517 20:09:27.603308    5139 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0517 20:09:27.603319    5139 kubeadm.go:156] found existing configuration files:

I0517 20:09:27.603540    5139 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0517 20:09:27.607525    5139 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0517 20:09:27.607665    5139 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0517 20:09:27.611308    5139 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0517 20:09:27.615434    5139 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0517 20:09:27.615548    5139 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0517 20:09:27.619300    5139 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0517 20:09:27.623033    5139 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0517 20:09:27.623157    5139 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0517 20:09:27.626779    5139 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0517 20:09:27.630628    5139 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0517 20:09:27.630747    5139 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0517 20:09:27.634412    5139 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0517 20:09:27.655767    5139 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0517 20:09:27.655868    5139 kubeadm.go:309] [preflight] Running pre-flight checks
I0517 20:09:27.707720    5139 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0517 20:09:27.707884    5139 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0517 20:09:27.708022    5139 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0517 20:09:27.820473    5139 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0517 20:09:27.832423    5139 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0517 20:09:27.832581    5139 kubeadm.go:309] [certs] Using existing ca certificate authority
I0517 20:09:27.832676    5139 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0517 20:09:27.832799    5139 kubeadm.go:309] [certs] Using existing apiserver-kubelet-client certificate and key on disk
I0517 20:09:27.832890    5139 kubeadm.go:309] [certs] Using existing front-proxy-ca certificate authority
I0517 20:09:27.832996    5139 kubeadm.go:309] [certs] Using existing front-proxy-client certificate and key on disk
I0517 20:09:27.833063    5139 kubeadm.go:309] [certs] Using existing etcd/ca certificate authority
I0517 20:09:27.833151    5139 kubeadm.go:309] [certs] Using existing etcd/server certificate and key on disk
I0517 20:09:27.833241    5139 kubeadm.go:309] [certs] Using existing etcd/peer certificate and key on disk
I0517 20:09:27.833354    5139 kubeadm.go:309] [certs] Using existing etcd/healthcheck-client certificate and key on disk
I0517 20:09:27.833452    5139 kubeadm.go:309] [certs] Using existing apiserver-etcd-client certificate and key on disk
I0517 20:09:27.833525    5139 kubeadm.go:309] [certs] Using the existing "sa" key
I0517 20:09:27.833623    5139 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0517 20:09:28.012296    5139 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0517 20:09:28.149193    5139 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0517 20:09:28.257467    5139 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0517 20:09:28.335829    5139 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0517 20:09:28.468317    5139 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0517 20:09:28.468639    5139 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0517 20:09:28.470483    5139 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0517 20:09:28.479166    5139 out.go:204]     ‚ñ™ Booting up control plane ...
I0517 20:09:28.479339    5139 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0517 20:09:28.479456    5139 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0517 20:09:28.479547    5139 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0517 20:09:28.481756    5139 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0517 20:09:28.483043    5139 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0517 20:09:28.483125    5139 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0517 20:09:28.553128    5139 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0517 20:09:28.553202    5139 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0517 20:09:29.058765    5139 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 505.769958ms
I0517 20:09:29.059038    5139 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0517 20:09:32.561395    5139 kubeadm.go:309] [api-check] The API server is healthy after 3.502396417s
I0517 20:09:32.569412    5139 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0517 20:09:32.574606    5139 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0517 20:09:32.586830    5139 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0517 20:09:32.587217    5139 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0517 20:09:32.590638    5139 kubeadm.go:309] [bootstrap-token] Using token: 7bumba.ebwz9kemfqupsyu4
I0517 20:09:32.608740    5139 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I0517 20:09:32.609128    5139 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0517 20:09:32.609222    5139 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0517 20:09:32.609389    5139 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0517 20:09:32.609786    5139 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0517 20:09:32.612459    5139 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0517 20:09:32.614223    5139 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0517 20:09:32.972256    5139 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0517 20:09:33.396750    5139 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0517 20:09:33.976996    5139 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0517 20:09:33.979273    5139 kubeadm.go:309] 
I0517 20:09:33.979354    5139 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0517 20:09:33.979359    5139 kubeadm.go:309] 
I0517 20:09:33.979511    5139 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0517 20:09:33.979517    5139 kubeadm.go:309] 
I0517 20:09:33.979599    5139 kubeadm.go:309]   mkdir -p $HOME/.kube
I0517 20:09:33.979749    5139 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0517 20:09:33.979898    5139 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0517 20:09:33.979929    5139 kubeadm.go:309] 
I0517 20:09:33.980174    5139 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0517 20:09:33.980178    5139 kubeadm.go:309] 
I0517 20:09:33.980320    5139 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0517 20:09:33.980324    5139 kubeadm.go:309] 
I0517 20:09:33.980387    5139 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0517 20:09:33.980470    5139 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0517 20:09:33.980580    5139 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0517 20:09:33.980584    5139 kubeadm.go:309] 
I0517 20:09:33.980683    5139 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0517 20:09:33.980843    5139 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0517 20:09:33.980849    5139 kubeadm.go:309] 
I0517 20:09:33.980961    5139 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token 7bumba.ebwz9kemfqupsyu4 \
I0517 20:09:33.981163    5139 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:7d3666d55eee475afaecdc2fbf9127eb6e3a387869ba37cf9c9dae8bfab274f0 \
I0517 20:09:33.981557    5139 kubeadm.go:309] 	--control-plane 
I0517 20:09:33.981562    5139 kubeadm.go:309] 
I0517 20:09:33.981661    5139 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0517 20:09:33.981664    5139 kubeadm.go:309] 
I0517 20:09:33.981756    5139 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token 7bumba.ebwz9kemfqupsyu4 \
I0517 20:09:33.981904    5139 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:7d3666d55eee475afaecdc2fbf9127eb6e3a387869ba37cf9c9dae8bfab274f0 
I0517 20:09:33.986631    5139 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0517 20:09:33.986882    5139 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0517 20:09:33.986903    5139 cni.go:84] Creating CNI manager for ""
I0517 20:09:33.986920    5139 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0517 20:09:33.992482    5139 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0517 20:09:34.002081    5139 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0517 20:09:34.017727    5139 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0517 20:09:34.030442    5139 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0517 20:09:34.030655    5139 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0517 20:09:34.031083    5139 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_05_17T20_09_34_0700 minikube.k8s.io/version=v1.33.1 minikube.k8s.io/commit=248d1ec5b3f9be5569977749a725f47b018078ff minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0517 20:09:34.036963    5139 ops.go:34] apiserver oom_adj: -16
I0517 20:09:34.302418    5139 kubeadm.go:1107] duration metric: took 271.972459ms to wait for elevateKubeSystemPrivileges
W0517 20:09:34.305836    5139 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0517 20:09:34.305863    5139 kubeadm.go:393] duration metric: took 9.889631875s to StartCluster
I0517 20:09:34.305887    5139 settings.go:142] acquiring lock: {Name:mkd828db7b7cf864e2d4d124744c6ee9c2490d8b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0517 20:09:34.306217    5139 settings.go:150] Updating kubeconfig:  /etc/kubernetes/admin.conf
I0517 20:09:34.312105    5139 out.go:177] 
W0517 20:09:34.315824    5139 out.go:239] ‚ùå  Exiting due to GUEST_START: failed to start node: Failed kubeconfig update: writing kubeconfig: Error creating directory: /etc/kubernetes: mkdir /etc/kubernetes: permission denied
W0517 20:09:34.315857    5139 out.go:239] 
W0517 20:09:34.318892    5139 out.go:239] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0517 20:09:34.329959    5139 out.go:177] 


==> Docker <==
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"format\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox format\": invalid key: \"format\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"format\\\"\". Proceed without further sandbox information."
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"format\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDformat\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"format\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"format\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"format\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDformat\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"format\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube cri-dockerd[1187]: time="2024-05-17T13:09:26Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
May 17 13:09:26 minikube dockerd[924]: time="2024-05-17T13:09:26.825272846Z" level=info msg="ignoring event" container=4c19f068b9c198a22806e722fea58f7ae862b4e127ea0fc21e9729ffd30ffa14 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 17 13:09:26 minikube dockerd[924]: time="2024-05-17T13:09:26.874533638Z" level=info msg="ignoring event" container=01c0910648084074d086b8a8db1a10c89d4f486ba9f370ef6b140bc1741aef8d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 17 13:09:26 minikube dockerd[924]: time="2024-05-17T13:09:26.922045054Z" level=info msg="ignoring event" container=08a050334feafc910fb332fa37f42f58e6dde256bbabfff847ac7c239eedcdbc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 17 13:09:26 minikube dockerd[924]: time="2024-05-17T13:09:26.973230054Z" level=info msg="ignoring event" container=a4da18ffdf29d2b138ceda67a116d7f37bb15c8323ae712872be20d8af650457 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 17 13:09:29 minikube cri-dockerd[1187]: time="2024-05-17T13:09:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/86ca242d3a71cbc544a00c95a69640c33b838bc02230f106cd965e371983b5ee/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 17 13:09:29 minikube cri-dockerd[1187]: time="2024-05-17T13:09:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6a6437c6baf087bfdde91b228624d2a635e2a25351daec6766371e0714b90dd8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 17 13:09:29 minikube cri-dockerd[1187]: time="2024-05-17T13:09:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6575cf3c95997464c98847d569b8c4fa3dd863a19d7b76be7a4339c670505089/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 17 13:09:29 minikube cri-dockerd[1187]: time="2024-05-17T13:09:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/874a9c09d42b643781c07a2e85f5e36ad887acd11e795a322c46c3cedde95644/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 17 13:09:49 minikube cri-dockerd[1187]: time="2024-05-17T13:09:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/27ca7523b73464352cbc9ffffbfc3e11685689ffacd502ddf7a3be5cae62d702/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 17 13:09:49 minikube cri-dockerd[1187]: time="2024-05-17T13:09:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1bb52491a7a53bf3f8a6539b2c95f36ff29ad1b9cef4c62779c06e068dd5e96f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 17 13:09:49 minikube cri-dockerd[1187]: time="2024-05-17T13:09:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/45eb8a794937afbd4950c7ebc65047d5fae28b4d2a79c3a013f29cb345546052/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 17 13:09:53 minikube cri-dockerd[1187]: time="2024-05-17T13:09:53Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
bf866fd346b9d       2437cf7621777       2 minutes ago       Running             coredns                   0                   45eb8a794937a       coredns-7db6d8ff4d-w5wtp
90c792e497dfa       2437cf7621777       2 minutes ago       Running             coredns                   0                   1bb52491a7a53       coredns-7db6d8ff4d-n7slm
8947731d8cc58       cb7eac0b42cc1       2 minutes ago       Running             kube-proxy                0                   27ca7523b7346       kube-proxy-7vkgc
fa3eb080e69b3       014faa467e297       3 minutes ago       Running             etcd                      0                   874a9c09d42b6       etcd-minikube
ddc26cc437975       547adae34140b       3 minutes ago       Running             kube-scheduler            0                   6575cf3c95997       kube-scheduler-minikube
c5f24d2f14693       68feac521c0f1       3 minutes ago       Running             kube-controller-manager   0                   6a6437c6baf08       kube-controller-manager-minikube
d14df8935602f       181f57fd3cdb7       3 minutes ago       Running             kube-apiserver            0                   86ca242d3a71c       kube-apiserver-minikube


==> coredns [90c792e497df] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2


==> coredns [bf866fd346b9] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=248d1ec5b3f9be5569977749a725f47b018078ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_17T20_09_34_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 17 May 2024 13:09:31 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 17 May 2024 13:12:37 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 17 May 2024 13:09:53 +0000   Fri, 17 May 2024 13:09:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 17 May 2024 13:09:53 +0000   Fri, 17 May 2024 13:09:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 17 May 2024 13:09:53 +0000   Fri, 17 May 2024 13:09:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 17 May 2024 13:09:53 +0000   Fri, 17 May 2024 13:09:33 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             4017032Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             4017032Ki
  pods:               110
System Info:
  Machine ID:                 08ca7d2a14ad4d4da211752ebdea1b5b
  System UUID:                08ca7d2a14ad4d4da211752ebdea1b5b
  Boot ID:                    d84bf17f-3ac5-47d5-850e-e6f252c55caa
  Kernel Version:             6.6.26-linuxkit
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-7db6d8ff4d-n7slm            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     2m53s
  kube-system                 coredns-7db6d8ff4d-w5wtp            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     2m53s
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         3m8s
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m8s
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m8s
  kube-system                 kube-proxy-7vkgc                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m53s
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m8s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%!)(MISSING)  0 (0%!)(MISSING)
  memory             240Mi (6%!)(MISSING)  340Mi (8%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 2m52s                  kube-proxy       
  Normal  Starting                 3m13s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  3m13s (x8 over 3m13s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m13s (x8 over 3m13s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m13s (x7 over 3m13s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  3m13s                  kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 3m8s                   kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  3m8s                   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m8s                   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m8s                   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeNotReady             3m8s                   kubelet          Node minikube status is now: NodeNotReady
  Normal  NodeAllocatableEnforced  3m8s                   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                3m8s                   kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           2m54s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May17 12:21] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.233807] netlink: 'init': attribute type 4 has an invalid length.
[  +0.034756] fakeowner: loading out-of-tree module taints kernel.
[  +0.009368] setting FUSE negative_dentry_timeout to 3600 seconds
[  +0.000005] setting FUSE entry_timeout to 3600 seconds
[  +0.000002] setting FUSE attr_timeout to 3600 seconds
[  +0.000001] ignoring STATX_ATIME
[  +0.000001] returning ENOSYS from FUSE_FLUSH
[  +0.000001] overriding FOPEN_KEEP_CACHE
[May17 12:23] systemd[629]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set


==> etcd [fa3eb080e69b] <==
{"level":"warn","ts":"2024-05-17T13:09:29.469882Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-05-17T13:09:29.470786Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-05-17T13:09:29.470841Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-05-17T13:09:29.470853Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-17T13:09:29.47088Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-17T13:09:29.471572Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-05-17T13:09:29.471629Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"arm64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-05-17T13:09:29.477449Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"5.635042ms"}
{"level":"info","ts":"2024-05-17T13:09:29.483366Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-05-17T13:09:29.48348Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-05-17T13:09:29.48352Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-05-17T13:09:29.483526Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-05-17T13:09:29.48353Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-05-17T13:09:29.483552Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-05-17T13:09:29.487094Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-05-17T13:09:29.489397Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-05-17T13:09:29.490432Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-05-17T13:09:29.491821Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-05-17T13:09:29.491897Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-05-17T13:09:29.492404Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-17T13:09:29.492438Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-17T13:09:29.492443Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-17T13:09:29.494727Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-17T13:09:29.494859Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-05-17T13:09:29.494874Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-05-17T13:09:29.495031Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-05-17T13:09:29.495073Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-17T13:09:29.495144Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-17T13:09:29.495154Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-17T13:09:30.484988Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-05-17T13:09:30.485032Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-05-17T13:09:30.485062Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-05-17T13:09:30.485084Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-05-17T13:09:30.485093Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-05-17T13:09:30.485102Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-05-17T13:09:30.485117Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-05-17T13:09:30.485768Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-05-17T13:09:30.485887Z","caller":"etcdserver/server.go:2578","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-17T13:09:30.486067Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-17T13:09:30.486498Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-17T13:09:30.486526Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-05-17T13:09:30.486526Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-17T13:09:30.486539Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-05-17T13:09:30.486567Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-17T13:09:30.486576Z","caller":"etcdserver/server.go:2602","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-17T13:09:30.488329Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-05-17T13:09:30.488333Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> kernel <==
 13:12:41 up 50 min,  0 users,  load average: 6.79, 53.36, 67.06
Linux minikube 6.6.26-linuxkit #1 SMP Sat Apr 27 04:13:19 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [d14df8935602] <==
I0517 13:09:31.114561       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0517 13:09:31.114696       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0517 13:09:31.114719       1 available_controller.go:423] Starting AvailableConditionController
I0517 13:09:31.114758       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0517 13:09:31.114823       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0517 13:09:31.114840       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0517 13:09:31.114856       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0517 13:09:31.114864       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0517 13:09:31.114875       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0517 13:09:31.115029       1 controller.go:116] Starting legacy_token_tracking_controller
I0517 13:09:31.115061       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0517 13:09:31.115092       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0517 13:09:31.115129       1 controller.go:139] Starting OpenAPI controller
I0517 13:09:31.115160       1 controller.go:87] Starting OpenAPI V3 controller
I0517 13:09:31.115710       1 naming_controller.go:291] Starting NamingConditionController
I0517 13:09:31.115774       1 establishing_controller.go:76] Starting EstablishingController
I0517 13:09:31.115815       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0517 13:09:31.115846       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0517 13:09:31.115904       1 crd_finalizer.go:266] Starting CRDFinalizer
I0517 13:09:31.115965       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0517 13:09:31.116129       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0517 13:09:31.116173       1 controller.go:78] Starting OpenAPI AggregationController
I0517 13:09:31.116200       1 aggregator.go:163] waiting for initial CRD sync...
I0517 13:09:31.116593       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0517 13:09:31.116674       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0517 13:09:31.116698       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0517 13:09:31.116725       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0517 13:09:31.116786       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0517 13:09:31.214759       1 shared_informer.go:320] Caches are synced for node_authorizer
I0517 13:09:31.214787       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0517 13:09:31.214798       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0517 13:09:31.214855       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0517 13:09:31.215553       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0517 13:09:31.215602       1 shared_informer.go:320] Caches are synced for configmaps
I0517 13:09:31.215628       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0517 13:09:31.215643       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0517 13:09:31.215712       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0517 13:09:31.215725       1 policy_source.go:224] refreshing policies
I0517 13:09:31.216733       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0517 13:09:31.216747       1 aggregator.go:165] initial CRD sync complete...
I0517 13:09:31.216754       1 autoregister_controller.go:141] Starting autoregister controller
I0517 13:09:31.216756       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0517 13:09:31.216759       1 cache.go:39] Caches are synced for autoregister controller
I0517 13:09:31.217367       1 controller.go:615] quota admission added evaluator for: namespaces
I0517 13:09:31.272850       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0517 13:09:32.125197       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0517 13:09:32.132973       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0517 13:09:32.132999       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0517 13:09:32.343209       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0517 13:09:32.355123       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0517 13:09:32.423712       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0517 13:09:32.426138       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0517 13:09:32.426761       1 controller.go:615] quota admission added evaluator for: endpoints
I0517 13:09:32.428554       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0517 13:09:33.305714       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0517 13:09:33.390048       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0517 13:09:33.395095       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0517 13:09:33.402047       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0517 13:09:48.639962       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0517 13:09:48.741093       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps


==> kube-controller-manager [c5f24d2f1469] <==
I0517 13:09:47.724798       1 shared_informer.go:320] Caches are synced for cronjob
I0517 13:09:47.731655       1 shared_informer.go:320] Caches are synced for crt configmap
I0517 13:09:47.733500       1 shared_informer.go:320] Caches are synced for TTL after finished
I0517 13:09:47.733529       1 shared_informer.go:320] Caches are synced for endpoint
I0517 13:09:47.735649       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0517 13:09:47.737627       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0517 13:09:47.737662       1 shared_informer.go:320] Caches are synced for job
I0517 13:09:47.737742       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0517 13:09:47.737765       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0517 13:09:47.740641       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0517 13:09:47.741578       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0517 13:09:47.741600       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0517 13:09:47.782836       1 shared_informer.go:320] Caches are synced for ephemeral
I0517 13:09:47.782889       1 shared_informer.go:320] Caches are synced for HPA
I0517 13:09:47.783729       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0517 13:09:47.785725       1 shared_informer.go:320] Caches are synced for expand
I0517 13:09:47.785773       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0517 13:09:47.785848       1 shared_informer.go:320] Caches are synced for PVC protection
I0517 13:09:47.785876       1 shared_informer.go:320] Caches are synced for ReplicationController
I0517 13:09:47.785899       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0517 13:09:47.786650       1 shared_informer.go:320] Caches are synced for deployment
I0517 13:09:47.787793       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0517 13:09:47.789501       1 shared_informer.go:320] Caches are synced for service account
I0517 13:09:47.792749       1 shared_informer.go:320] Caches are synced for PV protection
I0517 13:09:47.846145       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0517 13:09:47.879998       1 shared_informer.go:320] Caches are synced for node
I0517 13:09:47.880208       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0517 13:09:47.880233       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0517 13:09:47.880241       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0517 13:09:47.880248       1 shared_informer.go:320] Caches are synced for cidrallocator
I0517 13:09:47.884761       1 shared_informer.go:320] Caches are synced for GC
I0517 13:09:47.889422       1 range_allocator.go:381] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0517 13:09:47.890424       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0517 13:09:47.892921       1 shared_informer.go:320] Caches are synced for daemon sets
I0517 13:09:47.892957       1 shared_informer.go:320] Caches are synced for persistent volume
I0517 13:09:47.909705       1 shared_informer.go:320] Caches are synced for TTL
I0517 13:09:47.935665       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0517 13:09:47.936672       1 shared_informer.go:320] Caches are synced for attach detach
I0517 13:09:47.936731       1 shared_informer.go:320] Caches are synced for taint
I0517 13:09:47.936909       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0517 13:09:47.937040       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0517 13:09:47.937130       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0517 13:09:47.982532       1 shared_informer.go:320] Caches are synced for disruption
I0517 13:09:47.987656       1 shared_informer.go:320] Caches are synced for stateful set
I0517 13:09:47.998701       1 shared_informer.go:320] Caches are synced for resource quota
I0517 13:09:48.010861       1 shared_informer.go:320] Caches are synced for resource quota
I0517 13:09:48.422661       1 shared_informer.go:320] Caches are synced for garbage collector
I0517 13:09:48.433961       1 shared_informer.go:320] Caches are synced for garbage collector
I0517 13:09:48.434052       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0517 13:09:48.893158       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="250.982625ms"
I0517 13:09:48.901090       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="7.891167ms"
I0517 13:09:48.901148       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="35.5¬µs"
I0517 13:09:48.901164       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="9.667¬µs"
I0517 13:09:48.901853       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="17.875¬µs"
I0517 13:09:49.585817       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="67.292¬µs"
I0517 13:09:49.608739       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="25.625¬µs"
I0517 13:09:50.624945       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="5.854833ms"
I0517 13:09:50.626173       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="38.375¬µs"
I0517 13:09:50.636024       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="7.0495ms"
I0517 13:09:50.636076       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="24.875¬µs"


==> kube-proxy [8947731d8cc5] <==
I0517 13:09:49.442180       1 server_linux.go:69] "Using iptables proxy"
I0517 13:09:49.461332       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0517 13:09:49.484049       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0517 13:09:49.484076       1 server_linux.go:165] "Using iptables Proxier"
I0517 13:09:49.484897       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0517 13:09:49.484910       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0517 13:09:49.486006       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0517 13:09:49.486410       1 server.go:872] "Version info" version="v1.30.0"
I0517 13:09:49.486421       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0517 13:09:49.488777       1 config.go:192] "Starting service config controller"
I0517 13:09:49.488910       1 config.go:319] "Starting node config controller"
I0517 13:09:49.488963       1 config.go:101] "Starting endpoint slice config controller"
I0517 13:09:49.489465       1 shared_informer.go:313] Waiting for caches to sync for node config
I0517 13:09:49.489473       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0517 13:09:49.489493       1 shared_informer.go:313] Waiting for caches to sync for service config
I0517 13:09:49.591612       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0517 13:09:49.592200       1 shared_informer.go:320] Caches are synced for service config
I0517 13:09:49.592307       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [ddc26cc43797] <==
W0517 13:09:31.182784       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0517 13:09:31.193172       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0517 13:09:31.193193       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0517 13:09:31.194571       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0517 13:09:31.194647       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0517 13:09:31.194675       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0517 13:09:31.195171       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0517 13:09:31.196899       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0517 13:09:31.196930       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0517 13:09:31.196945       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0517 13:09:31.196949       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0517 13:09:31.196956       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0517 13:09:31.196977       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0517 13:09:31.197001       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0517 13:09:31.197015       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0517 13:09:31.197041       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0517 13:09:31.197061       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0517 13:09:31.197077       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0517 13:09:31.197120       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0517 13:09:31.197128       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0517 13:09:31.197155       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0517 13:09:31.197172       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0517 13:09:31.197175       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0517 13:09:31.197183       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0517 13:09:31.197188       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0517 13:09:31.197195       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0517 13:09:31.197202       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0517 13:09:31.197112       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0517 13:09:31.197094       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0517 13:09:31.197219       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0517 13:09:31.197116       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0517 13:09:31.197229       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0517 13:09:31.197232       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0517 13:09:31.197237       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0517 13:09:31.197251       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0517 13:09:31.197669       1 reflector.go:547] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0517 13:09:31.197702       1 reflector.go:150] runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0517 13:09:32.060981       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0517 13:09:32.061067       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0517 13:09:32.085590       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0517 13:09:32.085649       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0517 13:09:32.095108       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0517 13:09:32.095184       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0517 13:09:32.144164       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0517 13:09:32.144223       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0517 13:09:32.159235       1 reflector.go:547] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0517 13:09:32.159260       1 reflector.go:150] runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0517 13:09:32.181185       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0517 13:09:32.181212       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0517 13:09:32.210980       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0517 13:09:32.211009       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0517 13:09:32.221409       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0517 13:09:32.221437       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0517 13:09:32.269540       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0517 13:09:32.269608       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0517 13:09:32.270028       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0517 13:09:32.270046       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0517 13:09:32.289193       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0517 13:09:32.289222       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
I0517 13:09:34.395795       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 17 13:09:33 minikube kubelet[4799]: E0517 13:09:33.275780    4799 kubelet.go:2361] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.278657    4799 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.290632    4799 kubelet_node_status.go:112] "Node was previously registered" node="minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.290737    4799 kubelet_node_status.go:76] "Successfully registered node" node="minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.293912    4799 setters.go:580] "Node became not ready" node="minikube" condition={"type":"Ready","status":"False","lastHeartbeatTime":"2024-05-17T13:09:33Z","lastTransitionTime":"2024-05-17T13:09:33Z","reason":"KubeletNotReady","message":"container runtime status check may not have completed yet"}
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.300798    4799 cpu_manager.go:214] "Starting CPU manager" policy="none"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.300825    4799 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.300942    4799 state_mem.go:36] "Initialized new in-memory state store"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.301236    4799 state_mem.go:88] "Updated default CPUSet" cpuSet=""
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.301244    4799 state_mem.go:96] "Updated CPUSet assignments" assignments={}
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.301260    4799 policy_none.go:49] "None policy: Start"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.307531    4799 memory_manager.go:170] "Starting memorymanager" policy="None"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.307564    4799 state_mem.go:35] "Initializing new in-memory state store"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.307783    4799 state_mem.go:75] "Updated machine memory state"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.308680    4799 manager.go:479] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.309115    4799 container_log_manager.go:186] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.309274    4799 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.310225    4799 kubelet_node_status.go:497] "Fast updating node status as it just became ready"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.376972    4799 topology_manager.go:215] "Topology Admit Handler" podUID="063d6b9688927e601f52fd818d1305c5" podNamespace="kube-system" podName="etcd-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.377652    4799 topology_manager.go:215] "Topology Admit Handler" podUID="3c555f828409b009ebee39fdbedfcac0" podNamespace="kube-system" podName="kube-apiserver-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.377712    4799 topology_manager.go:215] "Topology Admit Handler" podUID="7fd44e8d11c3e0ffe6b1825e2a1f2270" podNamespace="kube-system" podName="kube-controller-manager-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.377823    4799 topology_manager.go:215] "Topology Admit Handler" podUID="f9c8e1d0d74b1727abdb4b4a31d3a7c1" podNamespace="kube-system" podName="kube-scheduler-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485562    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485606    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485624    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/063d6b9688927e601f52fd818d1305c5-etcd-data\") pod \"etcd-minikube\" (UID: \"063d6b9688927e601f52fd818d1305c5\") " pod="kube-system/etcd-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485641    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485657    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485670    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/f9c8e1d0d74b1727abdb4b4a31d3a7c1-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"f9c8e1d0d74b1727abdb4b4a31d3a7c1\") " pod="kube-system/kube-scheduler-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485689    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485706    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485721    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485737    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485750    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/063d6b9688927e601f52fd818d1305c5-etcd-certs\") pod \"etcd-minikube\" (UID: \"063d6b9688927e601f52fd818d1305c5\") " pod="kube-system/etcd-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485772    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485783    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485790    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
May 17 13:09:33 minikube kubelet[4799]: I0517 13:09:33.485801    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
May 17 13:09:34 minikube kubelet[4799]: I0517 13:09:34.172854    4799 apiserver.go:52] "Watching apiserver"
May 17 13:09:34 minikube kubelet[4799]: I0517 13:09:34.280679    4799 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
May 17 13:09:34 minikube kubelet[4799]: E0517 13:09:34.437263    4799 kubelet.go:1928] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
May 17 13:09:34 minikube kubelet[4799]: I0517 13:09:34.465075    4799 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.464786377 podStartE2EDuration="1.464786377s" podCreationTimestamp="2024-05-17 13:09:33 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-17 13:09:34.46279321 +0000 UTC m=+1.378068835" watchObservedRunningTime="2024-05-17 13:09:34.464786377 +0000 UTC m=+1.380062085"
May 17 13:09:34 minikube kubelet[4799]: I0517 13:09:34.465609    4799 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.46560046 podStartE2EDuration="1.46560046s" podCreationTimestamp="2024-05-17 13:09:33 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-17 13:09:34.448769835 +0000 UTC m=+1.364045418" watchObservedRunningTime="2024-05-17 13:09:34.46560046 +0000 UTC m=+1.380876001"
May 17 13:09:34 minikube kubelet[4799]: I0517 13:09:34.479052    4799 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.479038127 podStartE2EDuration="1.479038127s" podCreationTimestamp="2024-05-17 13:09:33 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-17 13:09:34.478937669 +0000 UTC m=+1.394213293" watchObservedRunningTime="2024-05-17 13:09:34.479038127 +0000 UTC m=+1.394313710"
May 17 13:09:34 minikube kubelet[4799]: I0517 13:09:34.487136    4799 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.4871117520000001 podStartE2EDuration="1.487111752s" podCreationTimestamp="2024-05-17 13:09:33 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-17 13:09:34.483063377 +0000 UTC m=+1.398339001" watchObservedRunningTime="2024-05-17 13:09:34.487111752 +0000 UTC m=+1.402387293"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.757047    4799 topology_manager.go:215] "Topology Admit Handler" podUID="f8fa0ad9-b466-4aec-bdd7-4e7ff9d5030b" podNamespace="kube-system" podName="kube-proxy-7vkgc"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.887693    4799 topology_manager.go:215] "Topology Admit Handler" podUID="57f63721-aec8-4c30-93b8-20dc8f3ea333" podNamespace="kube-system" podName="coredns-7db6d8ff4d-n7slm"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.893210    4799 topology_manager.go:215] "Topology Admit Handler" podUID="962ef8d1-6946-4cea-88a8-d62bd6e08ad4" podNamespace="kube-system" podName="coredns-7db6d8ff4d-w5wtp"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.897572    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fwfm7\" (UniqueName: \"kubernetes.io/projected/f8fa0ad9-b466-4aec-bdd7-4e7ff9d5030b-kube-api-access-fwfm7\") pod \"kube-proxy-7vkgc\" (UID: \"f8fa0ad9-b466-4aec-bdd7-4e7ff9d5030b\") " pod="kube-system/kube-proxy-7vkgc"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.897616    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-q9hjg\" (UniqueName: \"kubernetes.io/projected/57f63721-aec8-4c30-93b8-20dc8f3ea333-kube-api-access-q9hjg\") pod \"coredns-7db6d8ff4d-n7slm\" (UID: \"57f63721-aec8-4c30-93b8-20dc8f3ea333\") " pod="kube-system/coredns-7db6d8ff4d-n7slm"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.897630    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/962ef8d1-6946-4cea-88a8-d62bd6e08ad4-config-volume\") pod \"coredns-7db6d8ff4d-w5wtp\" (UID: \"962ef8d1-6946-4cea-88a8-d62bd6e08ad4\") " pod="kube-system/coredns-7db6d8ff4d-w5wtp"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.897660    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/f8fa0ad9-b466-4aec-bdd7-4e7ff9d5030b-kube-proxy\") pod \"kube-proxy-7vkgc\" (UID: \"f8fa0ad9-b466-4aec-bdd7-4e7ff9d5030b\") " pod="kube-system/kube-proxy-7vkgc"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.897671    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/f8fa0ad9-b466-4aec-bdd7-4e7ff9d5030b-lib-modules\") pod \"kube-proxy-7vkgc\" (UID: \"f8fa0ad9-b466-4aec-bdd7-4e7ff9d5030b\") " pod="kube-system/kube-proxy-7vkgc"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.897693    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-m9k26\" (UniqueName: \"kubernetes.io/projected/962ef8d1-6946-4cea-88a8-d62bd6e08ad4-kube-api-access-m9k26\") pod \"coredns-7db6d8ff4d-w5wtp\" (UID: \"962ef8d1-6946-4cea-88a8-d62bd6e08ad4\") " pod="kube-system/coredns-7db6d8ff4d-w5wtp"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.897707    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/f8fa0ad9-b466-4aec-bdd7-4e7ff9d5030b-xtables-lock\") pod \"kube-proxy-7vkgc\" (UID: \"f8fa0ad9-b466-4aec-bdd7-4e7ff9d5030b\") " pod="kube-system/kube-proxy-7vkgc"
May 17 13:09:48 minikube kubelet[4799]: I0517 13:09:48.897736    4799 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/57f63721-aec8-4c30-93b8-20dc8f3ea333-config-volume\") pod \"coredns-7db6d8ff4d-n7slm\" (UID: \"57f63721-aec8-4c30-93b8-20dc8f3ea333\") " pod="kube-system/coredns-7db6d8ff4d-n7slm"
May 17 13:09:49 minikube kubelet[4799]: I0517 13:09:49.585233    4799 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-7db6d8ff4d-n7slm" podStartSLOduration=1.5851787590000002 podStartE2EDuration="1.585178759s" podCreationTimestamp="2024-05-17 13:09:48 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-17 13:09:49.584763217 +0000 UTC m=+16.500038800" watchObservedRunningTime="2024-05-17 13:09:49.585178759 +0000 UTC m=+16.500454383"
May 17 13:09:49 minikube kubelet[4799]: I0517 13:09:49.599622    4799 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-7vkgc" podStartSLOduration=1.599593301 podStartE2EDuration="1.599593301s" podCreationTimestamp="2024-05-17 13:09:48 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-17 13:09:49.599532967 +0000 UTC m=+16.514808592" watchObservedRunningTime="2024-05-17 13:09:49.599593301 +0000 UTC m=+16.514868925"
May 17 13:09:50 minikube kubelet[4799]: I0517 13:09:50.619098    4799 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-7db6d8ff4d-w5wtp" podStartSLOduration=2.6190566349999997 podStartE2EDuration="2.619056635s" podCreationTimestamp="2024-05-17 13:09:48 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-17 13:09:49.608467551 +0000 UTC m=+16.523743175" watchObservedRunningTime="2024-05-17 13:09:50.619056635 +0000 UTC m=+17.534332301"
May 17 13:09:53 minikube kubelet[4799]: I0517 13:09:53.816175    4799 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
May 17 13:09:53 minikube kubelet[4799]: I0517 13:09:53.818725    4799 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"

